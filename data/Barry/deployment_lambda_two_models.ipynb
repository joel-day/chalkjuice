{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql as mysql\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# AWS Credentials & Region\n",
    "AWS_REGION = \"us-east-2\"  # Change to your region\n",
    "DATABASE = \"chalk\"\n",
    "TABLE = \"chalkjuice_data\"\n",
    "S3_OUTPUT = \"s3://chalkjuice/golden_athena/\"  # Replace with your actual S3 bucket\n",
    "\n",
    "# Initialize Athena Client\n",
    "athena_client = boto3.client(\"athena\", region_name=AWS_REGION)\n",
    "\n",
    "weights = [5,7,9,13,.3,.25,.25,.2]\n",
    "games_back = weights[0] + weights[1] + weights[2] + weights[3] + 1\n",
    "\n",
    "\n",
    "# AWS Credentials & Region\n",
    "AWS_REGION = \"us-east-2\"  # Change to your region\n",
    "DATABASE = \"chalk\"\n",
    "TABLE = \"chalkjuice_data\"\n",
    "S3_OUTPUT = \"s3://chalkjuice/golden_athena/\"  # Replace with your actual S3 bucket\n",
    "\n",
    "# Initialize Athena Client\n",
    "athena_client = boto3.client(\"athena\", region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_athena_df(query):\n",
    "    # Start Query Execution\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\"Database\": DATABASE},\n",
    "        ResultConfiguration={\"OutputLocation\": S3_OUTPUT},\n",
    "    )\n",
    "\n",
    "    # Get Query Execution ID\n",
    "    query_execution_id = response[\"QueryExecutionId\"]\n",
    "\n",
    "    # Wait for Query to Complete\n",
    "    while True:\n",
    "        status = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        state = status[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        \n",
    "        if state in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n",
    "            break\n",
    "        \n",
    "        time.sleep(.1)  # Check every .1 seconds\n",
    "\n",
    "    if state != \"SUCCEEDED\":\n",
    "        failure_reason = status[\"QueryExecution\"][\"Status\"].get(\"StateChangeReason\", \"Unknown Error\")\n",
    "        raise Exception(f\"Athena query failed with state: {state}, Reason: {failure_reason}\")\n",
    "\n",
    "\n",
    "    # Get Query Results\n",
    "    results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "\n",
    "    columns = [col[\"Label\"] for col in results[\"ResultSet\"][\"ResultSetMetadata\"][\"ColumnInfo\"]]\n",
    "\n",
    "    # Extract Rows\n",
    "    rows = []\n",
    "    for row in results[\"ResultSet\"][\"Rows\"][1:]:  # Skip header row\n",
    "        extracted_row = [col.get(\"VarCharValue\", None) for col in row[\"Data\"]]  # Extract actual values\n",
    "        rows.append(extracted_row)\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df = df.fillna(\"NA\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg(df, col, gb1, gb2, gb3, gb4, weight1, weight2, weight3, weight4, inte = None):\n",
    "\n",
    "\n",
    "    gb2 = gb1 + gb2\n",
    "    gb3 = gb2 + gb3\n",
    "    gb4 = gb3 + gb4\n",
    "\n",
    "    average_gb1 = df[col].iloc[:gb1].mean()\n",
    "    weighted_gb1 = average_gb1 * weight1\n",
    "\n",
    "    average_gb2 = df[col].iloc[gb1:gb2].mean()\n",
    "    weighted_gb2 = average_gb2 * weight2\n",
    "\n",
    "\n",
    "    average_gb3 = df[col].iloc[gb2:gb3].mean()\n",
    "    weighted_gb3 = average_gb3 * weight3\n",
    "\n",
    "    average_gb4 = df[col].iloc[gb3:gb4].mean()\n",
    "    weighted_gb4 = average_gb4 * weight4\n",
    "\n",
    "\n",
    "    weighted_avg = round(((weighted_gb1 + weighted_gb2 + weighted_gb3 + weighted_gb4) / sum([weight1, weight2, weight3, weight4])), 3)\n",
    "\n",
    "    \n",
    "    if inte == 1:\n",
    "        weighted_avg = int(weighted_avg)\n",
    "\n",
    "    return weighted_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This function that return the teams first game, and the oldest game that can be used for Barry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oldest_usable_game(team, games_back):\n",
    "    query = f\"\"\"\n",
    "        WITH ordered_games AS (\n",
    "            SELECT date,\n",
    "                ROW_NUMBER() OVER (PARTITION BY team ORDER BY date ASC) AS row_num\n",
    "            FROM \"{DATABASE}\".\"{TABLE}\"\n",
    "            WHERE team = '{team}' \n",
    "        )\n",
    "        SELECT date\n",
    "        FROM ordered_games\n",
    "    \"\"\"\n",
    "    df = query_athena_df(query)\n",
    "    df.columns = df.columns.str.replace('_', ' ').str.title()  # Format column names\n",
    "    df['Date'] = pd.to_datetime(df['Date'])  # Ensure 'Date' is in datetime format\n",
    "    df = df.sort_values(by='Date', ascending=True)  # Sort from oldest to newest\n",
    "\n",
    "    teams_first_game = str(df['Date'].iloc[(0)])[:10]\n",
    "    oldest_game_for_modeling = str(df['Date'].iloc[(games_back)])[:10]\n",
    "    \n",
    "    messege = f'Barry reserves the first 34 games of a teams history to use for modeling. The first superbowl era game {team} played was on {teams_first_game}, chose a date equal to or more recent than {oldest_game_for_modeling}.'\n",
    "    # the model used depends on the date due to missing data. \n",
    "\n",
    "\n",
    "    return team, teams_first_game, oldest_game_for_modeling, messege"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barry reserves the first 34 games of a teams history to use for modeling. The first superbowl era game HOU played was on 2002-09-08, chose a date equal to or more recent than 2004-09-26.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team, teams_first_game, oldest_game_for_modeling, messege = oldest_usable_game('HOU', 34)\n",
    "messege"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a new model that drops downs_3D_att\tdowns_3D_con\tdowns_4D_att\tdowns_4D_con\tgame_time_off becauser this data is missing in games begining 1991. So if the date for a team is before 1994 the code will defualt to this second smaller model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I am using the training set created in the 'barry_02212025.ipynb' script. i just removes the time of possesion and cluth columns 4 total. 2 off 2 def. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = pd.read_csv('aggregated_stats_modeling_removed_featues.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>passing_yds</th>\n",
       "      <th>passing_int</th>\n",
       "      <th>passing_times_sacked</th>\n",
       "      <th>rushing_yds</th>\n",
       "      <th>fmb</th>\n",
       "      <th>punts_yds</th>\n",
       "      <th>penalty_yds</th>\n",
       "      <th>def_passing_yds</th>\n",
       "      <th>def_passing_int</th>\n",
       "      <th>...</th>\n",
       "      <th>def_rushing_yds</th>\n",
       "      <th>def_fmb</th>\n",
       "      <th>pass_play_percentage</th>\n",
       "      <th>def_pass_play_percentage</th>\n",
       "      <th>drives</th>\n",
       "      <th>def_drives</th>\n",
       "      <th>tds_per_yard</th>\n",
       "      <th>def_tds_per_yard</th>\n",
       "      <th>fg_percentage</th>\n",
       "      <th>home_game</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>130.390</td>\n",
       "      <td>1.072</td>\n",
       "      <td>2.770</td>\n",
       "      <td>120.300</td>\n",
       "      <td>1.131</td>\n",
       "      <td>268.253</td>\n",
       "      <td>57.167</td>\n",
       "      <td>193.517</td>\n",
       "      <td>1.098</td>\n",
       "      <td>...</td>\n",
       "      <td>89.181</td>\n",
       "      <td>0.847</td>\n",
       "      <td>47.695</td>\n",
       "      <td>55.853</td>\n",
       "      <td>9.554</td>\n",
       "      <td>8.364</td>\n",
       "      <td>48.955</td>\n",
       "      <td>55.486</td>\n",
       "      <td>52.669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>130.668</td>\n",
       "      <td>0.903</td>\n",
       "      <td>2.696</td>\n",
       "      <td>121.437</td>\n",
       "      <td>0.849</td>\n",
       "      <td>256.991</td>\n",
       "      <td>52.673</td>\n",
       "      <td>180.224</td>\n",
       "      <td>0.619</td>\n",
       "      <td>...</td>\n",
       "      <td>126.402</td>\n",
       "      <td>0.609</td>\n",
       "      <td>48.000</td>\n",
       "      <td>47.742</td>\n",
       "      <td>9.430</td>\n",
       "      <td>9.431</td>\n",
       "      <td>48.492</td>\n",
       "      <td>65.779</td>\n",
       "      <td>61.184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>131.860</td>\n",
       "      <td>0.952</td>\n",
       "      <td>2.686</td>\n",
       "      <td>118.405</td>\n",
       "      <td>0.898</td>\n",
       "      <td>266.543</td>\n",
       "      <td>52.295</td>\n",
       "      <td>220.878</td>\n",
       "      <td>1.350</td>\n",
       "      <td>...</td>\n",
       "      <td>113.832</td>\n",
       "      <td>0.760</td>\n",
       "      <td>48.579</td>\n",
       "      <td>54.146</td>\n",
       "      <td>9.607</td>\n",
       "      <td>8.622</td>\n",
       "      <td>48.103</td>\n",
       "      <td>70.696</td>\n",
       "      <td>59.025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>132.273</td>\n",
       "      <td>1.169</td>\n",
       "      <td>1.638</td>\n",
       "      <td>131.074</td>\n",
       "      <td>0.610</td>\n",
       "      <td>226.376</td>\n",
       "      <td>41.846</td>\n",
       "      <td>199.622</td>\n",
       "      <td>1.285</td>\n",
       "      <td>...</td>\n",
       "      <td>146.990</td>\n",
       "      <td>0.629</td>\n",
       "      <td>47.094</td>\n",
       "      <td>48.406</td>\n",
       "      <td>8.470</td>\n",
       "      <td>9.106</td>\n",
       "      <td>40.777</td>\n",
       "      <td>70.031</td>\n",
       "      <td>69.603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>132.276</td>\n",
       "      <td>1.027</td>\n",
       "      <td>2.721</td>\n",
       "      <td>115.766</td>\n",
       "      <td>0.974</td>\n",
       "      <td>267.350</td>\n",
       "      <td>52.826</td>\n",
       "      <td>186.404</td>\n",
       "      <td>0.605</td>\n",
       "      <td>...</td>\n",
       "      <td>126.514</td>\n",
       "      <td>0.707</td>\n",
       "      <td>49.283</td>\n",
       "      <td>48.904</td>\n",
       "      <td>9.555</td>\n",
       "      <td>9.446</td>\n",
       "      <td>46.354</td>\n",
       "      <td>62.414</td>\n",
       "      <td>56.693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10817</th>\n",
       "      <td>31</td>\n",
       "      <td>324.114</td>\n",
       "      <td>1.080</td>\n",
       "      <td>1.648</td>\n",
       "      <td>109.169</td>\n",
       "      <td>0.272</td>\n",
       "      <td>198.134</td>\n",
       "      <td>53.776</td>\n",
       "      <td>218.148</td>\n",
       "      <td>1.037</td>\n",
       "      <td>...</td>\n",
       "      <td>125.005</td>\n",
       "      <td>0.682</td>\n",
       "      <td>62.070</td>\n",
       "      <td>51.483</td>\n",
       "      <td>9.252</td>\n",
       "      <td>10.207</td>\n",
       "      <td>77.213</td>\n",
       "      <td>58.294</td>\n",
       "      <td>73.334</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10818</th>\n",
       "      <td>16</td>\n",
       "      <td>324.790</td>\n",
       "      <td>1.032</td>\n",
       "      <td>1.503</td>\n",
       "      <td>111.804</td>\n",
       "      <td>0.352</td>\n",
       "      <td>198.991</td>\n",
       "      <td>55.949</td>\n",
       "      <td>267.263</td>\n",
       "      <td>0.932</td>\n",
       "      <td>...</td>\n",
       "      <td>107.553</td>\n",
       "      <td>0.588</td>\n",
       "      <td>61.738</td>\n",
       "      <td>57.522</td>\n",
       "      <td>9.194</td>\n",
       "      <td>9.504</td>\n",
       "      <td>78.564</td>\n",
       "      <td>76.311</td>\n",
       "      <td>74.040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10819</th>\n",
       "      <td>14</td>\n",
       "      <td>325.112</td>\n",
       "      <td>1.092</td>\n",
       "      <td>1.633</td>\n",
       "      <td>111.139</td>\n",
       "      <td>0.270</td>\n",
       "      <td>165.150</td>\n",
       "      <td>59.412</td>\n",
       "      <td>228.781</td>\n",
       "      <td>0.709</td>\n",
       "      <td>...</td>\n",
       "      <td>132.238</td>\n",
       "      <td>0.578</td>\n",
       "      <td>63.030</td>\n",
       "      <td>51.463</td>\n",
       "      <td>8.857</td>\n",
       "      <td>9.934</td>\n",
       "      <td>81.823</td>\n",
       "      <td>66.097</td>\n",
       "      <td>73.597</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10820</th>\n",
       "      <td>39</td>\n",
       "      <td>325.238</td>\n",
       "      <td>0.809</td>\n",
       "      <td>1.209</td>\n",
       "      <td>104.625</td>\n",
       "      <td>0.467</td>\n",
       "      <td>199.777</td>\n",
       "      <td>65.375</td>\n",
       "      <td>211.953</td>\n",
       "      <td>0.991</td>\n",
       "      <td>...</td>\n",
       "      <td>111.499</td>\n",
       "      <td>0.594</td>\n",
       "      <td>60.627</td>\n",
       "      <td>55.773</td>\n",
       "      <td>9.909</td>\n",
       "      <td>8.479</td>\n",
       "      <td>89.139</td>\n",
       "      <td>51.250</td>\n",
       "      <td>60.739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10821</th>\n",
       "      <td>26</td>\n",
       "      <td>326.612</td>\n",
       "      <td>1.107</td>\n",
       "      <td>1.886</td>\n",
       "      <td>104.448</td>\n",
       "      <td>0.261</td>\n",
       "      <td>194.227</td>\n",
       "      <td>53.286</td>\n",
       "      <td>237.426</td>\n",
       "      <td>1.436</td>\n",
       "      <td>...</td>\n",
       "      <td>100.963</td>\n",
       "      <td>1.303</td>\n",
       "      <td>62.457</td>\n",
       "      <td>59.091</td>\n",
       "      <td>9.199</td>\n",
       "      <td>8.791</td>\n",
       "      <td>78.822</td>\n",
       "      <td>53.602</td>\n",
       "      <td>73.986</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10822 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       points  passing_yds  passing_int  passing_times_sacked  rushing_yds  \\\n",
       "0           9      130.390        1.072                 2.770      120.300   \n",
       "1          26      130.668        0.903                 2.696      121.437   \n",
       "2          10      131.860        0.952                 2.686      118.405   \n",
       "3          14      132.273        1.169                 1.638      131.074   \n",
       "4          24      132.276        1.027                 2.721      115.766   \n",
       "...       ...          ...          ...                   ...          ...   \n",
       "10817      31      324.114        1.080                 1.648      109.169   \n",
       "10818      16      324.790        1.032                 1.503      111.804   \n",
       "10819      14      325.112        1.092                 1.633      111.139   \n",
       "10820      39      325.238        0.809                 1.209      104.625   \n",
       "10821      26      326.612        1.107                 1.886      104.448   \n",
       "\n",
       "         fmb  punts_yds  penalty_yds  def_passing_yds  def_passing_int  ...  \\\n",
       "0      1.131    268.253       57.167          193.517            1.098  ...   \n",
       "1      0.849    256.991       52.673          180.224            0.619  ...   \n",
       "2      0.898    266.543       52.295          220.878            1.350  ...   \n",
       "3      0.610    226.376       41.846          199.622            1.285  ...   \n",
       "4      0.974    267.350       52.826          186.404            0.605  ...   \n",
       "...      ...        ...          ...              ...              ...  ...   \n",
       "10817  0.272    198.134       53.776          218.148            1.037  ...   \n",
       "10818  0.352    198.991       55.949          267.263            0.932  ...   \n",
       "10819  0.270    165.150       59.412          228.781            0.709  ...   \n",
       "10820  0.467    199.777       65.375          211.953            0.991  ...   \n",
       "10821  0.261    194.227       53.286          237.426            1.436  ...   \n",
       "\n",
       "       def_rushing_yds  def_fmb  pass_play_percentage  \\\n",
       "0               89.181    0.847                47.695   \n",
       "1              126.402    0.609                48.000   \n",
       "2              113.832    0.760                48.579   \n",
       "3              146.990    0.629                47.094   \n",
       "4              126.514    0.707                49.283   \n",
       "...                ...      ...                   ...   \n",
       "10817          125.005    0.682                62.070   \n",
       "10818          107.553    0.588                61.738   \n",
       "10819          132.238    0.578                63.030   \n",
       "10820          111.499    0.594                60.627   \n",
       "10821          100.963    1.303                62.457   \n",
       "\n",
       "       def_pass_play_percentage  drives  def_drives  tds_per_yard  \\\n",
       "0                        55.853   9.554       8.364        48.955   \n",
       "1                        47.742   9.430       9.431        48.492   \n",
       "2                        54.146   9.607       8.622        48.103   \n",
       "3                        48.406   8.470       9.106        40.777   \n",
       "4                        48.904   9.555       9.446        46.354   \n",
       "...                         ...     ...         ...           ...   \n",
       "10817                    51.483   9.252      10.207        77.213   \n",
       "10818                    57.522   9.194       9.504        78.564   \n",
       "10819                    51.463   8.857       9.934        81.823   \n",
       "10820                    55.773   9.909       8.479        89.139   \n",
       "10821                    59.091   9.199       8.791        78.822   \n",
       "\n",
       "       def_tds_per_yard  fg_percentage  home_game  \n",
       "0                55.486         52.669          1  \n",
       "1                65.779         61.184          1  \n",
       "2                70.696         59.025          1  \n",
       "3                70.031         69.603          1  \n",
       "4                62.414         56.693          1  \n",
       "...                 ...            ...        ...  \n",
       "10817            58.294         73.334          0  \n",
       "10818            76.311         74.040          1  \n",
       "10819            66.097         73.597          1  \n",
       "10820            51.250         60.739          0  \n",
       "10821            53.602         73.986          1  \n",
       "\n",
       "[10822 rows x 21 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## outliers\n",
    "def handle_outliers(df, feature):\n",
    "\n",
    "    # calculate the mean and standard deviation of the feature\n",
    "    mean = df[feature].mean()\n",
    "    std = df[feature].std()\n",
    "\n",
    "    # define the threshold for outliers (3 standard deviations)\n",
    "    threshold = 3 * std\n",
    "\n",
    "    # save the indices of outliers\n",
    "    outlier_indices = df[(df[feature] < mean - threshold) | (df[feature] > mean + threshold)].index\n",
    "   \n",
    "   # replace outliers with NaN values\n",
    "    #df.loc[outlier_indices, feature] = np.nan\n",
    "    #print(f\"Number of rows dropped for feature '{feature}': {len(outlier_indices)}\")\n",
    "\n",
    "    # you can also remove outliers from the DataFrame completely\n",
    "    df = df.drop(outlier_indices)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, target):\n",
    "\n",
    "    # outliers\n",
    "    for feature in df.select_dtypes(include=[np.number]).columns:\n",
    "        df = handle_outliers(df, feature)\n",
    "    print(target)\n",
    "    # partitioning\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "    X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_remaining, y_remaining, test_size=0.5, random_state=42)\n",
    "\n",
    "    # scaling\n",
    "    standard_scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(standard_scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_val   = pd.DataFrame(standard_scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "    X_test  = pd.DataFrame(standard_scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    scaler_filename = 'chalk_22_scaler_2.pkl'\n",
    "    joblib.dump(standard_scaler, scaler_filename)\n",
    "\n",
    "\n",
    "    # feature importance\n",
    "    #model = RandomForestRegressor(random_state=42)\n",
    "    #clf = model.fit(X_train, y_train)\n",
    "    #feature_importance(clf, X_train)\n",
    "\n",
    "    # feature independece\n",
    "    numerical_cols = X_test.select_dtypes(include=[float, int])\n",
    "    results_list = []\n",
    "    # iterate over all combinations of numerical columns\n",
    "    for i, col1 in enumerate(numerical_cols.columns):\n",
    "        for col2 in numerical_cols.columns[i+1:]:\n",
    "            x = numerical_cols[col1]\n",
    "            y = numerical_cols[col2]\n",
    "            # calculate Pearson's correlation coefficient and p-value\n",
    "            corr_coefficient, p_value = pearsonr(x, y)\n",
    "            # append the results to the list\n",
    "            results_list.append({'Variable1': col1, 'Variable2': col2, 'Correlation Coefficient': corr_coefficient, 'P-Value': p_value})\n",
    " \n",
    "    # convert the list to a DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    results_df = results_df.sort_values(by='Correlation Coefficient', ascending=False)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_val, y_val, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points\n"
     ]
    }
   ],
   "source": [
    "df = model_2\n",
    "target = 'points'\n",
    "X_train, X_test, y_train, y_test, X_val, y_val, results_df = preprocessing(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lr_model_2.joblib']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "joblib.dump(model, 'lr_model_2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "Scaler expects 20 features.\n",
      "Expected number of features: 20\n"
     ]
    }
   ],
   "source": [
    "# Load the scaler\n",
    "scaler = joblib.load(\"chalk_22_scaler_2.pkl\")\n",
    "\n",
    "# Check if it's a StandardScaler object\n",
    "print(type(scaler))  \n",
    "\n",
    "# Try accessing feature-related attributes\n",
    "if hasattr(scaler, \"n_features_in_\"):\n",
    "    print(f\"Scaler expects {scaler.n_features_in_} features.\")  # Only works for newer versions of scikit-learn\n",
    "else:\n",
    "    print(\"Scaler does not have `n_features_in_`, checking alternative methods.\")\n",
    "\n",
    "# Check expected shape\n",
    "print(f\"Expected number of features: {scaler.mean_.shape[0]}\")  # Mean vector length = number of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model\n",
    "## Create dataframe one row per team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_df_for_each_matchup(team, opponent, date1, date2, model):\n",
    "    \n",
    "    # create data frames with the 35 most recent games =< the proivided date for both team offense and opponent defense\n",
    "    query_offense = f'''\n",
    "        SELECT date, team, opponent, points, home_game,\n",
    "            passing_yds, passing_tds, passing_int, \n",
    "            passing_times_sacked, rushing_yds, \n",
    "            rush_tds, fmb, \"3d_att\", \"3d_conversions\", \"4d_att\", \"4d_conversions\", time_of_possession, \n",
    "            fga, punts_yds, punts_total, \"2pm\", \"2pa\",\n",
    "            penalty_yds, fgm, passing_att, rushing_att\n",
    "        FROM \"{DATABASE}\".\"{TABLE}\"\n",
    "        WHERE team = '{team}'\n",
    "            AND TRY_CAST(DATE_PARSE(date, '%m/%d/%Y') AS DATE) <= DATE '{date1}'\n",
    "        ORDER BY TRY_CAST(DATE_PARSE(date, '%m/%d/%Y') AS DATE) DESC\n",
    "        LIMIT {games_back+1};\n",
    "    '''\n",
    "    off_df = query_athena_df(query_offense)\n",
    "\n",
    "    query_defense = f'''\n",
    "        SELECT \n",
    "            date, team, opponent, passing_yds AS def_passing_yds, \n",
    "            passing_tds AS def_passing_tds, passing_int AS def_passing_int, \n",
    "            passing_times_sacked AS def_passing_times_sacked, rushing_yds AS def_rushing_yds, \n",
    "            rush_tds AS def_rush_tds, fmb AS def_fmb, \n",
    "            \"3d_att\" AS def_3d_att, \"3d_conversions\" AS def_3d_conversions, \n",
    "            \"4d_att\" AS def_4d_att, \"4d_conversions\" AS def_4d_conversions, \n",
    "            time_of_possession AS def_time_of_possession, fga AS def_fga, \n",
    "            punts_total AS def_punts_total, \"2pm\" AS def_2pm, \n",
    "            \"2pa\" AS def_2pa, passing_att AS def_passing_att, rushing_att AS def_rushing_att\n",
    "        FROM \"{DATABASE}\".\"{TABLE}\"\n",
    "        WHERE opponent = '{opponent}'\n",
    "            AND TRY_CAST(DATE_PARSE(date, '%m/%d/%Y') AS DATE) <= DATE '{date2}' \n",
    "        ORDER BY TRY_CAST(DATE_PARSE(date, '%m/%d/%Y') AS DATE) DESC\n",
    "        LIMIT {games_back+1};\n",
    "    '''\n",
    "    def_df = query_athena_df(query_defense)\n",
    "\n",
    "\n",
    "    # join the two dfs on the index column because teams may play on different days on the same week. \n",
    "    merged_df = off_df.merge(def_df, left_index=True, right_index=True, how='inner')\n",
    "    merged_df = merged_df[merged_df['date_x'] != '1/2/2023']\n",
    "    merged_df = merged_df[merged_df['date_y'] != '1/2/2023']\n",
    "\n",
    "    # first take out any information from the row date in question\n",
    "    home_game = int(merged_df['home_game'][0])\n",
    "\n",
    "    # remove the top row because you dont want to train the model on data from the same week we are uses for points\n",
    "    merged_df_2 = merged_df.drop(merged_df.index[0])\n",
    "    merged_df_2 = merged_df_2.drop(columns=['date_x', 'team_x', 'opponent_x', 'date_y', 'team_y', 'opponent_y', 'home_game', 'points'])\n",
    "\n",
    "    if model == 2:\n",
    "        # drop these for the smaller model\n",
    "        merged_df_2 = merged_df_2.drop(columns=['time_of_possession', 'def_time_of_possession', '2pm', '2pa', 'def_2pm', 'def_2pa',\n",
    "            '3d_att', 'def_3d_att', '3d_conversions', 'def_3d_conversions', '4d_att', 'def_4d_att', '4d_conversions', \n",
    "            'def_4d_conversions'])\n",
    "\n",
    "    # convert to integers. this is the part that despises N/A values \n",
    "    merged_df_2 = merged_df_2.astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "    return(merged_df_2, home_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, model):  \n",
    "\n",
    "    merged_df_2 = df\n",
    "    ##### pass_play_percentage\n",
    "    merged_df_2['pass_play_percentage'] = 100*(merged_df_2['passing_att'] / (merged_df_2['passing_att'] + merged_df_2['rushing_att']))\n",
    "    merged_df_2['def_pass_play_percentage'] = 100*(merged_df_2['def_passing_att'] / (merged_df_2['def_passing_att'] + merged_df_2['def_rushing_att']))\n",
    "\n",
    "    merged_df_2 = merged_df_2.drop(columns=['passing_att', 'rushing_att', 'def_passing_att', 'def_rushing_att'])\n",
    "\n",
    "    ##### drives\n",
    "    # Offensive drives\n",
    "    merged_df_2['drives'] = merged_df_2['passing_tds'] + merged_df_2['rush_tds'] + merged_df_2['fga'] + merged_df_2['punts_total']\n",
    "\n",
    "    # Defensive drives\n",
    "    merged_df_2['def_drives'] = merged_df_2['def_passing_tds'] + merged_df_2['def_rush_tds'] + merged_df_2['def_fga'] + merged_df_2['def_punts_total']\n",
    "\n",
    "    # drop\n",
    "    merged_df_2 = merged_df_2.drop(columns=['punts_total', 'def_punts_total', 'def_fga'])\n",
    "\n",
    "    ##### tds per 10000 yards\n",
    "    # Offensive touchdowns per yard\n",
    "    merged_df_2['tds_per_yard'] = 10000 * ((merged_df_2['passing_tds'] + merged_df_2['rush_tds']) / \\\n",
    "                                (merged_df_2['passing_yds'] + merged_df_2['rushing_yds']))\n",
    "\n",
    "    # Defensive touchdowns per yard\n",
    "    merged_df_2['def_tds_per_yard'] = 10000 * ((merged_df_2['def_passing_tds'] + merged_df_2['def_rush_tds']) / \\\n",
    "                                    (merged_df_2['def_passing_yds'] + merged_df_2['def_rushing_yds']))\n",
    "\n",
    "    merged_df_2 = merged_df_2.drop(columns=['passing_tds', 'rush_tds', 'def_passing_tds', 'def_rush_tds'])\n",
    "\n",
    "\n",
    "    ##### fg_percentage\n",
    "    merged_df_2['fg_percentage'] = 100*(np.where(merged_df_2['fga'] == 0, 0, merged_df_2['fgm'] / merged_df_2['fga']))\n",
    "\n",
    "    # Now drop 'fgm' and 'fga'\n",
    "    merged_df_2 = merged_df_2.drop(columns=['fgm', 'fga'])\n",
    "\n",
    "    merged_df_2 = merged_df_2.astype(int)\n",
    "\n",
    "\n",
    "    if model == 1:\n",
    "        ##### cluth metric\n",
    "        # Offensive clutch conversion percentage\n",
    "        merged_df_2['clutch_conversion_percentage'] = 100*((merged_df_2['3d_conversions'] + merged_df_2['4d_conversions'] + merged_df_2['2pm']) / \\\n",
    "                                                    (merged_df_2['3d_att'] + merged_df_2['4d_att'] + merged_df_2['2pa']))\n",
    "\n",
    "        # Defensive clutch conversion percentage\n",
    "        merged_df_2['def_clutch_conversion_percentage'] = 100*(1 - ((merged_df_2['def_3d_conversions'] + merged_df_2['def_4d_conversions'] + merged_df_2['def_2pm']) / \\\n",
    "                                                        (merged_df_2['def_3d_att'] + merged_df_2['def_4d_att'] + merged_df_2['def_2pa'])))\n",
    "\n",
    "        # Drop the original columns\n",
    "        merged_df_2 = merged_df_2.drop(columns=[\n",
    "            '3d_att', '4d_att', '2pa', '3d_conversions', '4d_conversions', '2pm',\n",
    "            'def_3d_att', 'def_4d_att', 'def_2pa', 'def_3d_conversions', 'def_4d_conversions', 'def_2pm'\n",
    "        ])\n",
    "\n",
    "    return merged_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted(merged_df_2, model):    \n",
    "    # Dictionary to store weighted averages\n",
    "    weighted_averages = {}\n",
    "    # List of columns to calculate weighted averages for\n",
    "    if model == 2:\n",
    "        columns = [\n",
    "            'passing_yds', 'passing_int', 'passing_times_sacked', 'rushing_yds', 'fmb',\n",
    "            'punts_yds', 'penalty_yds', 'def_passing_yds',\n",
    "            'def_passing_int', 'def_passing_times_sacked',\n",
    "            'def_rushing_yds', 'def_fmb', 'def_passing_times_sacked',\n",
    "            'pass_play_percentage', 'def_pass_play_percentage', 'drives',\n",
    "            'def_drives', 'tds_per_yard', 'def_tds_per_yard',\n",
    "            'fg_percentage'\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        columns = [\n",
    "            'passing_yds', 'passing_int', 'passing_times_sacked', 'rushing_yds', 'fmb', 'time_of_possession',\n",
    "            'punts_yds', 'penalty_yds', 'def_passing_yds',\n",
    "            'def_passing_int', 'def_passing_times_sacked',\n",
    "            'def_rushing_yds', 'def_fmb', 'def_time_of_possession', 'def_passing_times_sacked',\n",
    "            'pass_play_percentage', 'def_pass_play_percentage', 'drives',\n",
    "            'def_drives', 'tds_per_yard', 'def_tds_per_yard',\n",
    "            'clutch_conversion_percentage', 'def_clutch_conversion_percentage',\n",
    "            'fg_percentage'\n",
    "            ]\n",
    "\n",
    "    # Calculate weighted averages and store in the dictionary\n",
    "    for col in columns:\n",
    "        weighted_averages[col] = weighted_avg(merged_df_2, col, *weights)\n",
    "\n",
    "    # Convert dictionary to a DataFrame (single-row)\n",
    "    weighted_avg_df = pd.DataFrame([weighted_averages])\n",
    "\n",
    "    return weighted_avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_2(team, opponent, date1, date2):  \n",
    "\n",
    "    team = team\n",
    "    opponent = opponent\n",
    "    date1 = date1\n",
    "    date2 = date2\n",
    "\n",
    "\n",
    "    if datetime.strptime(date1, '%Y-%m-%d').year < 1994 or datetime.strptime(date2, '%Y-%m-%d').year < 1994:\n",
    "        model_used = 2\n",
    "    else:\n",
    "        model_used = 1\n",
    "    print(model_used)\n",
    "\n",
    "\n",
    "\n",
    "    # create a 34 most recent game df fro each matchup\n",
    "    merged_df_2, home_game = collect_df_for_each_matchup(team, opponent, date1, date2, model_used)\n",
    "\n",
    "    # create features\n",
    "    merged_df_2 = create_features(merged_df_2, model_used)\n",
    "\n",
    "    # get the aggregated weighted averages of each feature\n",
    "    weighted_avg_df = weighted(merged_df_2, model_used)\n",
    "    weighted_avg_df.columns\n",
    "\n",
    "    # add home_game the only varible that doesnt need to be weighted\n",
    "    weighted_avg_df['home_game'] = home_game\n",
    "\n",
    "\n",
    "    if model_used == 2 :\n",
    "        # scale the featurs using the scaler form the model\n",
    "        scaler = joblib.load(\"chalk_22_scaler_2.pkl\")\n",
    "        scaled_inputs = scaler.transform(weighted_avg_df)\n",
    "        # model_2_training = pd.DataFrame(scaled_inputs, columns= model_2_training.columns)\n",
    "        model_lr = joblib.load(\"lr_model_2.joblib\")\n",
    "\n",
    "    else:\n",
    "        # scale the featurs using the scaler form the model\n",
    "        scaler = joblib.load(\"chalk_22_scaler.pkl\")\n",
    "        scaled_inputs = scaler.transform(weighted_avg_df)\n",
    "        # model_2_training = pd.DataFrame(scaled_inputs, columns= model_2_training.columns)\n",
    "        model_lr = joblib.load(\"lr_model.joblib\")\n",
    "\n",
    "\n",
    "    # Make predictions for each row\n",
    "    predictions = model_lr.predict(scaled_inputs)\n",
    "\n",
    "    return predictions, model_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output(team, opponent, points_team1, points_team2):    \n",
    "    sd = 8\n",
    "    limit = 100\n",
    "    n = 0\n",
    "    team1_wins = 0\n",
    "    team2_wins = 0\n",
    "\n",
    "    # Create an empty DataFrame to store results\n",
    "    df = pd.DataFrame(columns=[team, opponent])\n",
    "\n",
    "    while n < limit:\n",
    "        # Add some variance to the scores\n",
    "        team1_score =  round(np.random.normal(loc=points_team1.item(), scale=sd), 1)\n",
    "        team2_score =  round(np.random.normal(loc=points_team2.item(), scale=sd), 1)\n",
    "\n",
    "        # Append the scores to the DataFrame\n",
    "        df.loc[len(df)] = [team1_score, team2_score]\n",
    "\n",
    "        if team1_score > team2_score:\n",
    "            team1_wins += 1\n",
    "        else:\n",
    "            team2_wins += 1\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    team1_win_pct = team1_wins / limit\n",
    "\n",
    "    return team1_win_pct, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def week_to_date(team, season, week):\n",
    "    original_week = week\n",
    "    direction = -1  # Start by decrementing\n",
    "    \n",
    "    while 1 <= week <= 18:  # Ensure week stays within valid range\n",
    "        query_date_from_week = f'''\n",
    "            SELECT date\n",
    "            FROM \"{DATABASE}\".\"{TABLE}\"\n",
    "            WHERE team = '{team}' AND season = {season} AND week = {week}\n",
    "        '''\n",
    "        date_from_week = query_athena_df(query_date_from_week)\n",
    "        \n",
    "        if not date_from_week.empty:  # If the query returns a result, return it\n",
    "\n",
    "            date_from_week[\"date\"] = pd.to_datetime(date_from_week[\"date\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            return date_from_week[\"date\"].iloc[0]  # Returns '2002-10-06'\n",
    "\n",
    "        \n",
    "        week += direction  # Move in the current direction\n",
    "        \n",
    "        if week == 0:  # If we hit week 0, switch direction to increment\n",
    "            week = original_week + 1\n",
    "            direction = 1  # Start incrementing instead\n",
    "        \n",
    "    return None  # Return None if no valid date is found within week 1-18\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save row 1 column values into individual variables before dropping\n",
    "team = 'KAN'\n",
    "opponent = 'HOU'\n",
    "season1 = 2002\n",
    "week1 = 5\n",
    "season2 = 2002\n",
    "week2 = 9\n",
    "date1 = week_to_date(team, season1, week1)\n",
    "date2 = week_to_date(opponent, season2, week2)\n",
    "\n",
    "#date1 = '2010-10-08'\n",
    "#date2 = '2006-10-08'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2002-10-06'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2002-11-03'"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barry reserves the first 34 games of a teams history to use for modeling. The first superbowl era game HOU played was on 2002-09-08, chose a date equal to or more recent than 2004-09-26.\n"
     ]
    }
   ],
   "source": [
    "team, teams_first_game, oldest_game_for_modeling, messege = oldest_usable_game(team, 34)\n",
    "opponent, teams_first_game_2, oldest_game_for_modeling_2, messege_2 = oldest_usable_game(opponent, 34)\n",
    "\n",
    "# Convert input dates and oldest usable game dates to datetime objects\n",
    "date1_dt = datetime.strptime(date1, '%Y-%m-%d')\n",
    "date2_dt = datetime.strptime(date2, '%Y-%m-%d')\n",
    "oldest_game_for_modeling_dt = datetime.strptime(oldest_game_for_modeling, '%Y-%m-%d')\n",
    "oldest_game_for_modeling_2_dt = datetime.strptime(oldest_game_for_modeling_2, '%Y-%m-%d')\n",
    "\n",
    "\n",
    "# Check if either date is before the oldest usable game for that team\n",
    "date1_too_early = date1_dt < oldest_game_for_modeling_dt\n",
    "date2_too_early = date2_dt < oldest_game_for_modeling_2_dt\n",
    "\n",
    "if date1_too_early:\n",
    "    print(messege)\n",
    "if date2_too_early:\n",
    "    print(messege_2)\n",
    "\n",
    "# Only run predictions if BOTH dates are valid\n",
    "if not date1_too_early and not date2_too_early:\n",
    "    points_team1, model_used1 = get_predictions_2(team, opponent, date1, date2)\n",
    "    points_team2, model_used2 = get_predictions_2(opponent, team, date2, date1)\n",
    "    team1_win_pct, df = model_output(team, opponent, points_team1, points_team2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KAN</th>\n",
       "      <th>NWE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.2</td>\n",
       "      <td>29.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.7</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.9</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.8</td>\n",
       "      <td>26.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.7</td>\n",
       "      <td>31.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>25.7</td>\n",
       "      <td>30.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>11.9</td>\n",
       "      <td>33.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>11.5</td>\n",
       "      <td>38.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>12.4</td>\n",
       "      <td>20.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>28.2</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     KAN   NWE\n",
       "0   26.2  29.8\n",
       "1   14.7  21.0\n",
       "2    3.9  14.1\n",
       "3   24.8  26.9\n",
       "4   17.7  31.4\n",
       "..   ...   ...\n",
       "95  25.7  30.1\n",
       "96  11.9  33.2\n",
       "97  11.5  38.5\n",
       "98  12.4  20.1\n",
       "99  28.2  18.0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a table for dynamono db that has rows date, week, season, team, opponent, then all the required rows for modeling already averaged out for games back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
