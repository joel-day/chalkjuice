{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "import zipfile\n",
    "import json\n",
    "import hashlib\n",
    "import zlib\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "import itertools\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# AWS Credentials & Region\n",
    "AWS_REGION = \"us-east-2\"  # Change to your region\n",
    "DATABASE = \"chalk\"\n",
    "TABLE = \"chalkjuice_data\"\n",
    "S3_OUTPUT = \"s3://chalkjuice/golden_athena/\"  # Replace with your actual S3 bucket\n",
    "\n",
    "# Initialize Athena Client\n",
    "athena_client = boto3.client(\"athena\", region_name=AWS_REGION)\n",
    "\n",
    "weights = [5,7,9,13,.3,.25,.25,.2]\n",
    "games_back = weights[0] + weights[1] + weights[2] + weights[3] + 1\n",
    "\n",
    "\n",
    "# AWS Credentials & Region\n",
    "AWS_REGION = \"us-east-2\"  # Change to your region\n",
    "DATABASE = \"chalk\"\n",
    "TABLE = \"chalkjuice_data\"\n",
    "S3_OUTPUT = \"s3://chalkjuice/golden_athena/\"  # Replace with your actual S3 bucket\n",
    "\n",
    "# Initialize Athena Client\n",
    "athena_client = boto3.client(\"athena\", region_name=AWS_REGION)\n",
    "\n",
    "# Global model and scaler\n",
    "s3_bucket = \"chalkjuice\"\n",
    "model_key = \"lr_model.joblib\"\n",
    "model_key2 = \"lr_model_2.joblib\"\n",
    "scaler_key = \"chalk_22_scaler.pkl\"\n",
    "scaler_key2 = \"chalk_22_scaler_2.pkl\"\n",
    "model1 = None\n",
    "scaler1 = None\n",
    "model2 = None\n",
    "scaler2 = None\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    global model1, scaler1, model2, scaler2\n",
    "    if model1 is None or scaler1 is None or model2 is None or scaler2 is None:\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        model_path = \"lr_model.joblib\"\n",
    "        scaler_path = \"chalk_22_scaler.pkl\"\n",
    "        model_path2 = \"lr_model_2.joblib\"\n",
    "        scaler_path2 = \"chalk_22_scaler_2.pkl\"\n",
    "\n",
    "        # Download once per cold start\n",
    "        s3.download_file(s3_bucket, model_key, model_path)\n",
    "        s3.download_file(s3_bucket, scaler_key, scaler_path)\n",
    "        s3.download_file(s3_bucket, model_key2, model_path2)\n",
    "        s3.download_file(s3_bucket, scaler_key2, scaler_path2)\n",
    "\n",
    "        \n",
    "        model1 = joblib.load(model_path)\n",
    "        model2 = joblib.load(model_path2)\n",
    "        scaler1 = joblib.load(scaler_path)\n",
    "        scaler2 = joblib.load(scaler_path2)\n",
    "\n",
    "\n",
    "load_model()  # Load once when Lambda is cold\n",
    "\n",
    "def weighted_avg(df, col, gb1, gb2, gb3, gb4, weight1, weight2, weight3, weight4, inte = None):\n",
    "\n",
    "    # gb stands for games back \n",
    "    gb2 = gb1 + gb2\n",
    "    gb3 = gb2 + gb3\n",
    "    gb4 = gb3 + gb4\n",
    "\n",
    "    average_gb1 = df[col].iloc[:gb1].mean()\n",
    "    weighted_gb1 = average_gb1 * weight1\n",
    "\n",
    "    average_gb2 = df[col].iloc[gb1:gb2].mean()\n",
    "    weighted_gb2 = average_gb2 * weight2\n",
    "\n",
    "\n",
    "    average_gb3 = df[col].iloc[gb2:gb3].mean()\n",
    "    weighted_gb3 = average_gb3 * weight3\n",
    "\n",
    "    average_gb4 = df[col].iloc[gb3:gb4].mean()\n",
    "    weighted_gb4 = average_gb4 * weight4\n",
    "\n",
    "\n",
    "    weighted_avg = round(((weighted_gb1 + weighted_gb2 + weighted_gb3 + weighted_gb4) / sum([weight1, weight2, weight3, weight4])), 3)\n",
    "\n",
    "    \n",
    "    if inte == 1:\n",
    "        weighted_avg = int(weighted_avg)\n",
    "\n",
    "    return weighted_avg\n",
    "\n",
    "def query_athena_df(query):\n",
    "    # Start Query Execution\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\"Database\": DATABASE},\n",
    "        ResultConfiguration={\"OutputLocation\": S3_OUTPUT},\n",
    "    )\n",
    "\n",
    "    # Get Query Execution ID\n",
    "    query_execution_id = response[\"QueryExecutionId\"]\n",
    "\n",
    "    # Wait for Query to Complete\n",
    "    while True:\n",
    "        status = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        state = status[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        \n",
    "        if state in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n",
    "            break\n",
    "        \n",
    "        time.sleep(.1)  # Check every .1 seconds\n",
    "\n",
    "    if state != \"SUCCEEDED\":\n",
    "        failure_reason = status[\"QueryExecution\"][\"Status\"].get(\"StateChangeReason\", \"Unknown Error\")\n",
    "        raise Exception(f\"Athena query failed with state: {state}, Reason: {failure_reason}\")\n",
    "\n",
    "\n",
    "    # Get Query Results\n",
    "    results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "\n",
    "    columns = [col[\"Label\"] for col in results[\"ResultSet\"][\"ResultSetMetadata\"][\"ColumnInfo\"]]\n",
    "\n",
    "    # Extract Rows\n",
    "    rows = []\n",
    "    for row in results[\"ResultSet\"][\"Rows\"][1:]:  # Skip header row\n",
    "        extracted_row = [col.get(\"VarCharValue\", None) for col in row[\"Data\"]]  # Extract actual values\n",
    "        rows.append(extracted_row)\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df = df.fillna(\"NA\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def collect_df_for_each_matchup(team, opponent, date1, date2, model):\n",
    "    \n",
    "    # create data frames with the 35 most recent games =< the proivided date for both team offense and opponent defense\n",
    "    query_offense = f'''\n",
    "        SELECT date, team, opponent, points, home_game,\n",
    "            passing_yds, passing_tds, passing_int, \n",
    "            passing_times_sacked, rushing_yds, \n",
    "            rush_tds, fmb, \"3d_att\", \"3d_conversions\", \"4d_att\", \"4d_conversions\", time_of_possession, \n",
    "            fga, punts_yds, punts_total, \"2pm\", \"2pa\",\n",
    "            penalty_yds, fgm, passing_att, rushing_att\n",
    "        FROM \"{DATABASE}\".\"{TABLE}\"\n",
    "        WHERE team = '{team}'\n",
    "            AND TRY_CAST(DATE_PARSE(date, '%m/%d/%Y') AS DATE) <= DATE '{date1}'\n",
    "        ORDER BY TRY_CAST(DATE_PARSE(date, '%m/%d/%Y') AS DATE) DESC\n",
    "        LIMIT {games_back+1};\n",
    "    '''\n",
    "    off_df = query_athena_df(query_offense)\n",
    "\n",
    "    query_defense = f'''\n",
    "        SELECT \n",
    "            date, team, opponent, passing_yds AS def_passing_yds, \n",
    "            passing_tds AS def_passing_tds, passing_int AS def_passing_int, \n",
    "            passing_times_sacked AS def_passing_times_sacked, rushing_yds AS def_rushing_yds, \n",
    "            rush_tds AS def_rush_tds, fmb AS def_fmb, \n",
    "            \"3d_att\" AS def_3d_att, \"3d_conversions\" AS def_3d_conversions, \n",
    "            \"4d_att\" AS def_4d_att, \"4d_conversions\" AS def_4d_conversions, \n",
    "            time_of_possession AS def_time_of_possession, fga AS def_fga, \n",
    "            punts_total AS def_punts_total, \"2pm\" AS def_2pm, \n",
    "            \"2pa\" AS def_2pa, passing_att AS def_passing_att, rushing_att AS def_rushing_att\n",
    "        FROM \"{DATABASE}\".\"{TABLE}\"\n",
    "        WHERE opponent = '{opponent}'\n",
    "            AND TRY_CAST(DATE_PARSE(date, '%m/%d/%Y') AS DATE) <= DATE '{date2}' \n",
    "        ORDER BY TRY_CAST(DATE_PARSE(date, '%m/%d/%Y') AS DATE) DESC\n",
    "        LIMIT {games_back+1};\n",
    "    '''\n",
    "    def_df = query_athena_df(query_defense)\n",
    "\n",
    "\n",
    "    # join the two dfs on the index column because teams may play on different days on the same week. \n",
    "    merged_df = off_df.merge(def_df, left_index=True, right_index=True, how='inner')\n",
    "    merged_df = merged_df[merged_df['date_x'] != '1/2/2023']\n",
    "    merged_df = merged_df[merged_df['date_y'] != '1/2/2023']\n",
    "\n",
    "    # first take out any information from the row date in question\n",
    "    home_game = int(merged_df['home_game'][0])\n",
    "\n",
    "    # remove the top row because you dont want to train the model on data from the same week we are uses for points\n",
    "    merged_df_2 = merged_df.drop(merged_df.index[0])\n",
    "    merged_df_2 = merged_df_2.drop(columns=['date_x', 'team_x', 'opponent_x', 'date_y', 'team_y', 'opponent_y', 'home_game', 'points'])\n",
    "\n",
    "    if model == 2:\n",
    "        # drop these for the smaller model\n",
    "        merged_df_2 = merged_df_2.drop(columns=['time_of_possession', 'def_time_of_possession', '2pm', '2pa', 'def_2pm', 'def_2pa',\n",
    "            '3d_att', 'def_3d_att', '3d_conversions', 'def_3d_conversions', '4d_att', 'def_4d_att', '4d_conversions', \n",
    "            'def_4d_conversions'])\n",
    "\n",
    "    # convert to integers. this is the part that despises N/A values \n",
    "    merged_df_2 = merged_df_2.astype(int)\n",
    "\n",
    "\n",
    "    \n",
    "    return(merged_df_2, home_game)\n",
    "\n",
    "def create_features(df, model):  \n",
    "\n",
    "    merged_df_2 = df\n",
    "    ##### pass_play_percentage\n",
    "    merged_df_2['pass_play_percentage'] = 100*(merged_df_2['passing_att'] / (merged_df_2['passing_att'] + merged_df_2['rushing_att']))\n",
    "    merged_df_2['def_pass_play_percentage'] = 100*(merged_df_2['def_passing_att'] / (merged_df_2['def_passing_att'] + merged_df_2['def_rushing_att']))\n",
    "\n",
    "    merged_df_2 = merged_df_2.drop(columns=['passing_att', 'rushing_att', 'def_passing_att', 'def_rushing_att'])\n",
    "\n",
    "    ##### drives\n",
    "    # Offensive drives\n",
    "    merged_df_2['drives'] = merged_df_2['passing_tds'] + merged_df_2['rush_tds'] + merged_df_2['fga'] + merged_df_2['punts_total']\n",
    "\n",
    "    # Defensive drives\n",
    "    merged_df_2['def_drives'] = merged_df_2['def_passing_tds'] + merged_df_2['def_rush_tds'] + merged_df_2['def_fga'] + merged_df_2['def_punts_total']\n",
    "\n",
    "    # drop\n",
    "    merged_df_2 = merged_df_2.drop(columns=['punts_total', 'def_punts_total', 'def_fga'])\n",
    "\n",
    "    ##### tds per 10000 yards\n",
    "    # Offensive touchdowns per yard\n",
    "    merged_df_2['tds_per_yard'] = 10000 * ((merged_df_2['passing_tds'] + merged_df_2['rush_tds']) / \\\n",
    "                                (merged_df_2['passing_yds'] + merged_df_2['rushing_yds']))\n",
    "\n",
    "    # Defensive touchdowns per yard\n",
    "    merged_df_2['def_tds_per_yard'] = 10000 * ((merged_df_2['def_passing_tds'] + merged_df_2['def_rush_tds']) / \\\n",
    "                                    (merged_df_2['def_passing_yds'] + merged_df_2['def_rushing_yds']))\n",
    "\n",
    "    merged_df_2 = merged_df_2.drop(columns=['passing_tds', 'rush_tds', 'def_passing_tds', 'def_rush_tds'])\n",
    "\n",
    "\n",
    "    ##### fg_percentage\n",
    "    merged_df_2['fg_percentage'] = 100*(np.where(merged_df_2['fga'] == 0, 0, merged_df_2['fgm'] / merged_df_2['fga']))\n",
    "\n",
    "    # Now drop 'fgm' and 'fga'\n",
    "    merged_df_2 = merged_df_2.drop(columns=['fgm', 'fga'])\n",
    "\n",
    "    merged_df_2 = merged_df_2.astype(int)\n",
    "\n",
    "\n",
    "    if model == 1:\n",
    "        ##### cluth metric\n",
    "        # Offensive clutch conversion percentage\n",
    "        merged_df_2['clutch_conversion_percentage'] = 100*((merged_df_2['3d_conversions'] + merged_df_2['4d_conversions'] + merged_df_2['2pm']) / \\\n",
    "                                                    (merged_df_2['3d_att'] + merged_df_2['4d_att'] + merged_df_2['2pa']))\n",
    "\n",
    "        # Defensive clutch conversion percentage\n",
    "        merged_df_2['def_clutch_conversion_percentage'] = 100*(1 - ((merged_df_2['def_3d_conversions'] + merged_df_2['def_4d_conversions'] + merged_df_2['def_2pm']) / \\\n",
    "                                                        (merged_df_2['def_3d_att'] + merged_df_2['def_4d_att'] + merged_df_2['def_2pa'])))\n",
    "\n",
    "        # Drop the original columns\n",
    "        merged_df_2 = merged_df_2.drop(columns=[\n",
    "            '3d_att', '4d_att', '2pa', '3d_conversions', '4d_conversions', '2pm',\n",
    "            'def_3d_att', 'def_4d_att', 'def_2pa', 'def_3d_conversions', 'def_4d_conversions', 'def_2pm'\n",
    "        ])\n",
    "\n",
    "    return merged_df_2\n",
    "\n",
    "def weighted(merged_df_2, model):    \n",
    "    # Dictionary to store weighted averages\n",
    "    weighted_averages = {}\n",
    "    # List of columns to calculate weighted averages for\n",
    "    if model == 2:\n",
    "        columns = [\n",
    "            'passing_yds', 'passing_int', 'passing_times_sacked', 'rushing_yds', 'fmb',\n",
    "            'punts_yds', 'penalty_yds', 'def_passing_yds',\n",
    "            'def_passing_int', 'def_passing_times_sacked',\n",
    "            'def_rushing_yds', 'def_fmb', 'def_passing_times_sacked',\n",
    "            'pass_play_percentage', 'def_pass_play_percentage', 'drives',\n",
    "            'def_drives', 'tds_per_yard', 'def_tds_per_yard',\n",
    "            'fg_percentage'\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        columns = [\n",
    "            'passing_yds', 'passing_int', 'passing_times_sacked', 'rushing_yds', 'fmb', 'time_of_possession',\n",
    "            'punts_yds', 'penalty_yds', 'def_passing_yds',\n",
    "            'def_passing_int', 'def_passing_times_sacked',\n",
    "            'def_rushing_yds', 'def_fmb', 'def_time_of_possession', 'def_passing_times_sacked',\n",
    "            'pass_play_percentage', 'def_pass_play_percentage', 'drives',\n",
    "            'def_drives', 'tds_per_yard', 'def_tds_per_yard',\n",
    "            'clutch_conversion_percentage', 'def_clutch_conversion_percentage',\n",
    "            'fg_percentage'\n",
    "            ]\n",
    "\n",
    "    # Calculate weighted averages and store in the dictionary\n",
    "    for col in columns:\n",
    "        weighted_averages[col] = weighted_avg(merged_df_2, col, *weights)\n",
    "\n",
    "    # Convert dictionary to a DataFrame (single-row)\n",
    "    weighted_avg_df = pd.DataFrame([weighted_averages])\n",
    "\n",
    "    return weighted_avg_df\n",
    "\n",
    "def get_predictions_2(team, opponent, date1, date2):  \n",
    "\n",
    "    team = team\n",
    "    opponent = opponent\n",
    "    date1 = date1\n",
    "    date2 = date2\n",
    "\n",
    "\n",
    "    if datetime.strptime(date1, '%Y-%m-%d').year < 1994 or datetime.strptime(date2, '%Y-%m-%d').year < 1994:\n",
    "        model_used = 2\n",
    "  \n",
    "    else:\n",
    "        model_used = 1\n",
    "        \n",
    "    print(model_used)\n",
    "\n",
    "    # create a 34 most recent game df fro each matchup\n",
    "    merged_df_2, home_game = collect_df_for_each_matchup(team, opponent, date1, date2, model_used)\n",
    "\n",
    "    # create features\n",
    "    merged_df_2 = create_features(merged_df_2, model_used)\n",
    "\n",
    "    # get the aggregated weighted averages of each feature\n",
    "    weighted_avg_df = weighted(merged_df_2, model_used)\n",
    "    weighted_avg_df.columns\n",
    "\n",
    "    # add home_game the only varible that doesnt need to be weighted\n",
    "    weighted_avg_df['home_game'] = home_game\n",
    "\n",
    "\n",
    "    if model_used == 2 :\n",
    "        # scale the featurs using the scaler form the model\n",
    "        scaled_inputs = scaler2.transform(weighted_avg_df)\n",
    "        predictions = model2.predict(scaled_inputs)\n",
    "\n",
    "    else:\n",
    "        # scale the featurs using the scaler form the model\n",
    "        scaled_inputs = scaler1.transform(weighted_avg_df)\n",
    "        predictions = model1.predict(scaled_inputs)\n",
    "\n",
    "    return predictions, model_used\n",
    "\n",
    "def model_output(team, opponent, points_team1, points_team2, week1, week2, season1, season2):    \n",
    "    sd = 8\n",
    "    limit = 100\n",
    "    n = 0\n",
    "    team1_wins = 0\n",
    "    team2_wins = 0\n",
    "\n",
    "    # Create an empty DataFrame to store results\n",
    "    df = pd.DataFrame(columns=[f\"{team} ({season1},{week1})\", f\"{opponent} ({season2},{week2})\"])\n",
    "\n",
    "    while n < limit:\n",
    "        # Add some variance to the scores\n",
    "\n",
    "        team1_score = max(0, round(np.random.normal(loc=points_team1.item(), scale=sd), 0))\n",
    "        team2_score = max(0, round(np.random.normal(loc=points_team2.item(), scale=sd), 0))\n",
    "\n",
    "        # Append the scores to the DataFrame\n",
    "        df.loc[len(df)] = [team1_score, team2_score]\n",
    "\n",
    "        if team1_score > team2_score:\n",
    "            team1_wins += 1\n",
    "        else:\n",
    "            team2_wins += 1\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    team1_win_pct = team1_wins / limit\n",
    "\n",
    "    return team1_win_pct, df\n",
    "\n",
    "def oldest_usable_game(team, games_back):\n",
    "    query = f\"\"\"\n",
    "        WITH ordered_games AS (\n",
    "            SELECT date, week,\n",
    "                ROW_NUMBER() OVER (PARTITION BY team ORDER BY date ASC) AS row_num\n",
    "            FROM \"{DATABASE}\".\"{TABLE}\"\n",
    "            WHERE team = '{team}' \n",
    "        )\n",
    "        SELECT date, week\n",
    "        FROM ordered_games\n",
    "    \"\"\"\n",
    "    df = query_athena_df(query)\n",
    "    df.columns = df.columns.str.replace('_', ' ').str.title()  # Format column names\n",
    "    df['Date'] = pd.to_datetime(df['Date'])  # Ensure 'Date' is in datetime format\n",
    "    df = df.sort_values(by='Date', ascending=True)  # Sort from oldest to newest\n",
    "\n",
    "    teams_first_game = str(df['Date'].iloc[(0)])[:10]\n",
    "    season = str(df['Date'].iloc[(games_back)])[:4]\n",
    "    oldest_game_for_modeling = str(df['Date'].iloc[(games_back)])[:10]\n",
    "    oldest_week_for_modeling = str(df['Week'].iloc[(games_back)])\n",
    "\n",
    "\n",
    "    \n",
    "    messege = f'Barry reserves the first 34 games of a teams history to use for modeling. The first superbowl era game {team} played was on {teams_first_game}, make the selections for {team} equal to or more recent than Week {oldest_week_for_modeling}, {season}.'\n",
    "    # the model used depends on the date due to missing data. \n",
    "\n",
    "\n",
    "    return team, teams_first_game, oldest_game_for_modeling, oldest_week_for_modeling, messege\n",
    "\n",
    "def week_to_date(team, season, week):\n",
    "    week = int(week)\n",
    "    original_week = week\n",
    "    direction = -1  # Start by decrementing\n",
    "    \n",
    "    while 1 <= week <= 18:  # Ensure week stays within valid range\n",
    "        query_date_from_week = f'''\n",
    "            SELECT date\n",
    "            FROM \"{DATABASE}\".\"{TABLE}\"\n",
    "            WHERE team = '{team}' AND season = {season} AND week = {week}\n",
    "        '''\n",
    "        date_from_week = query_athena_df(query_date_from_week)\n",
    "        \n",
    "        if not date_from_week.empty:  # If the query returns a result, return it\n",
    "\n",
    "            date_from_week[\"date\"] = pd.to_datetime(date_from_week[\"date\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            return date_from_week[\"date\"].iloc[0]  # Returns '2002-10-06'\n",
    "\n",
    "        \n",
    "        week += direction  # Move in the current direction\n",
    "        \n",
    "        if week == 0:  # If we hit week 0, switch direction to increment\n",
    "            week = original_week + 1\n",
    "            direction = 1  # Start incrementing instead\n",
    "        \n",
    "    return None  # Return None if no valid date is found within week 1-18\n",
    "\n",
    "def date_to_week(team, season, date):\n",
    "\n",
    "    query_week = f'''\n",
    "        SELECT week\n",
    "        FROM \"{DATABASE}\".\"{TABLE}\"\n",
    "        WHERE team = '{team}' AND season = {season} AND date = '{date}\n",
    "    '''\n",
    "    week_from_date = query_athena_df(query_week)\n",
    "\n",
    "\n",
    "\n",
    "    return week_from_date\n",
    "\n",
    "def send_chunk(connection_id, message):\n",
    "    # Initialize API Gateway Client\n",
    "    api_gateway_endpoint = \"https://0t9yhsvorj.execute-api.us-east-2.amazonaws.com/production\"\n",
    "    api_client = boto3.client(\"apigatewaymanagementapi\", endpoint_url=api_gateway_endpoint)\n",
    "\n",
    "    \"\"\"Send JSON chunk via WebSocket\"\"\"\n",
    "    api_client.post_to_connection(ConnectionId=connection_id, Data=json.dumps(message).encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "CHUNK_SIZE = 50  # Number of rows per JSON chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN NWE 1 1 1990 2023\n"
     ]
    }
   ],
   "source": [
    "team = 'MIN'\n",
    "opponent = 'NWE'\n",
    "week1 = 1\n",
    "week2 = 1\n",
    "season1 = 1990\n",
    "season2 = 2023\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(team, opponent, week1, week2, season1, season2)\n",
    "   \n",
    "date1 = week_to_date(team, season1, week1)\n",
    "date2 = week_to_date(opponent, season2, week2)\n",
    "\n",
    "\n",
    "\n",
    "team, teams_first_game, oldest_game_for_modeling, oldest_week_for_modeling, messege = oldest_usable_game(team, 34)\n",
    "opponent, teams_first_game_2, oldest_game_for_modeling_2, oldest_week_for_modeling2, messege_2 = oldest_usable_game(opponent, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input dates and oldest usable game dates to datetime objects\n",
    "date1_dt = datetime.strptime(date1, '%Y-%m-%d')\n",
    "date2_dt = datetime.strptime(date2, '%Y-%m-%d')\n",
    "oldest_game_for_modeling_dt = datetime.strptime(oldest_game_for_modeling, '%Y-%m-%d')\n",
    "oldest_game_for_modeling_2_dt = datetime.strptime(oldest_game_for_modeling_2, '%Y-%m-%d')\n",
    "\n",
    "\n",
    "# Check if either date is before the oldest usable game for that team\n",
    "date1_too_early = date1_dt < oldest_game_for_modeling_dt\n",
    "date2_too_early = date2_dt < oldest_game_for_modeling_2_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "points_team1, model_used1 = get_predictions_2(team, opponent, date1, date2)\n",
    "points_team2, model_used2 = get_predictions_2(opponent, team, date2, date1)\n",
    "team1_win_pct, df = model_output(team, opponent, points_team1, points_team2, week1, week2, season1, season2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Game #'] = df.index + 1\n",
    "df = df[['Game #'] + [col for col in df.columns if col != 'Game #']]\n",
    "df = df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'connection_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m send_chunk(\u001b[43mconnection_id\u001b[49m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_results_team1_win_pct\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: team1_win_pct})\n\u001b[0;32m      3\u001b[0m csv_buffer \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mStringIO()\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(csv_buffer, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Save without index\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'connection_id' is not defined"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "    send_chunk(connection_id, {\"label\": \"model_results_team1_win_pct\", \"data\": team1_win_pct})\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)  # Save without index\n",
    "    csv_buffer.seek(0)\n",
    "    csv_reader = csv.reader(csv_buffer)\n",
    "    print(csv_reader)\n",
    "\n",
    "    headers = next(csv_reader)  # Extract headers from the first row\n",
    "\n",
    "    # Send headers separately\n",
    "    send_chunk(connection_id, {\"label\": \"model_results_headers\", \"data\": headers})\n",
    "\n",
    "    filtered_rows = list(itertools.islice(csv_reader, None))\n",
    "\n",
    "    chunk = []\n",
    "    count = 0\n",
    "\n",
    "    for row in filtered_rows:\n",
    "        chunk.append(row)  # Keep rows as lists (not dicts)\n",
    "        count += 1\n",
    "\n",
    "        # Send data in chunks\n",
    "        if count >= CHUNK_SIZE:\n",
    "            send_chunk(connection_id, {\"label\": \"model_results_rows\", \"data\":chunk})\n",
    "            chunk = [] \n",
    "            count = 0\n",
    "\n",
    "    send_chunk(connection_id, {\"label\": \"model_results_rows_last\", \"data\": chunk})\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    return {\"statusCode\": 200, \"body\": \"Streaming Complete\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
