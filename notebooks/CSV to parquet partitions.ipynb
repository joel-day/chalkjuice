{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to parquet partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\joel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.38.22)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.22 in c:\\users\\joel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3) (1.38.22)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\joel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in c:\\users\\joel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\joel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from botocore<1.39.0,>=1.38.22->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\joel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from botocore<1.39.0,>=1.38.22->boto3) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.39.0,>=1.38.22->boto3) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "'aws' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Login to AWS CLI and test connection\n",
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"chalkjuice\"                    # csv bucket\n",
    "csv_key = \"golden.csv\"                      # csv file name\n",
    "partitioned_folder = \"golden_partitions/\"   # Define new S3 folder for new partitions\n",
    "partition_by = \"season\"                     # Define column to partition by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_df_from_s3(): \n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # Create S3 resource\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "\n",
    "    # Read CSV from S3 int pandas\n",
    "    obj = s3.get_object(Bucket=s3_bucket, Key=csv_key)\n",
    "    df = pd.read_csv(obj[\"Body\"])\n",
    "\n",
    "    return df, s3_resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_files(df, partition_by):\n",
    "    # Loop through each unique year\n",
    "    for year in df[partition_by].unique():\n",
    "        # Filter data for that year\n",
    "        df_year = df[df[partition_by] == year]\n",
    "\n",
    "        df_year = df_year.drop(columns=[partition_by])\n",
    "\n",
    "        # Convert to Parquet using BytesIO (correct for binary files)\n",
    "        # Dont use StringIO(), which is meant for handling text data (like CSV)\n",
    "        buffer = io.BytesIO()\n",
    "        df_year.to_parquet(buffer, index=False, engine=\"pyarrow\")\n",
    "\n",
    "        # Move buffer position to the beginning\n",
    "        buffer.seek(0)\n",
    "\n",
    "        # Define S3 key (file path)\n",
    "        parquet_key = f\"{partitioned_folder}season={year}/data.parquet\"\n",
    "\n",
    "        # Upload to S3\n",
    "        s3_resource.Object(s3_bucket, parquet_key).put(Body=buffer)\n",
    "\n",
    "        print(f\"Uploaded {parquet_key} to S3 âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return the s3 csv as a pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, s3_resource = pull_df_from_s3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manualy change datatypes. Athena is fussy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'week', 'team', 'opponent', 'result', 'Points',\n",
       "       'points_allowed', 'overtime', 'home_game', 'passing_com', 'passing_att',\n",
       "       'passing_yds', 'passing_tds', 'passing_int', 'passing_times_sacked',\n",
       "       'passing_sack_yards', 'rushing_att', 'rushing_yds', 'rush_tds', 'fmb',\n",
       "       '3D_att', '3D_conversions', '4D_att', '4D_conversions',\n",
       "       'Time_of_possession', 'XPM', 'XPA', 'FGM', 'FGA', 'total_penalties',\n",
       "       'penalty_yds', 'punts_total', 'punts_yds', 'punts_blocks', '2PM', '2PA',\n",
       "       'safety', 'XPR', 'Pick_6', 'tds_fmb', 'tds_KR', 'tds_PR',\n",
       "       'tds_blocked_fg', 'tds_blocked_punt', 'tds_walkoff', 'tds_other',\n",
       "       '1D_passes', '1D_runs', 'weekday', 'season', 'game_duration_minutes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to nullable integer type (Int64)\n",
    "int_columns = [\n",
    "        'week', 'Points',\n",
    "        'points_allowed', 'overtime', 'home_game', 'passing_com', 'passing_att',\n",
    "        'passing_yds', 'passing_tds', 'passing_int', 'passing_times_sacked',\n",
    "        'passing_sack_yards', 'rushing_att', 'rushing_yds', 'rush_tds', 'fmb',\n",
    "        '3D_att', '3D_conversions', '4D_att', '4D_conversions',\n",
    "        'Time_of_possession', 'XPM', 'XPA', 'FGM', 'FGA', 'total_penalties',\n",
    "        'penalty_yds', 'punts_total', 'punts_yds', 'punts_blocks', '2PM', '2PA',\n",
    "        'safety', 'XPR', 'Pick_6', 'tds_fmb', 'tds_KR', 'tds_PR', 'tds_blocked_fg',\n",
    "        'tds_blocked_punt', 'tds_walkoff', 'tds_other', '1D_passes', '1D_runs',\n",
    "        'season', 'game_duration_minutes'\n",
    "]\n",
    "df[int_columns] = df[int_columns].astype(\"Int64\")\n",
    "\n",
    "# Assign string columns\n",
    "str_columns = [\n",
    "    'date', 'team', 'opponent', 'result', 'weekday'\n",
    "]\n",
    "df[str_columns] = df[str_columns].astype(\"string\")\n",
    "\n",
    "# Save date column as an object\n",
    "df[\"date\"] = df[\"date\"].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create partitioned Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_parquet_files(df, partition_by)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
