{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages.helpers.helpers import joel_boto\n",
    "import pandas as pd\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle data types for athena\n",
    "def convert_dtypes_for_athena(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        dtype = df[col].dtype\n",
    "        \n",
    "        if dtype in [\"int64\", \"float64\"]:\n",
    "            df[col] = df[col].astype(\"Int64\")  # Nullable integer\n",
    "        elif \"date\" in col_lower:\n",
    "            df[col] = df[col].astype(\"object\")  # Will convert to datetime64[ns]\n",
    "        elif dtype == 'object':\n",
    "            df[col] = df[col].astype(\"string\")  # Use StringDtype\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates parguet files from a df with specific partitioning and destination location\n",
    "def df_to_s3_parquet(s3_resource, df, s3_bucket, partition_by, partitioned_folder, local_parquet_file_path):\n",
    "    first = True  # Flag to track the first file\n",
    "\n",
    "    # Loop through each unique year\n",
    "    for year in df[partition_by].unique():\n",
    "        # Filter data for that year\n",
    "        df_year = df[df[partition_by] == year]\n",
    "\n",
    "        df_year = df_year.drop(columns=[partition_by])\n",
    "\n",
    "        # Convert to Parquet using BytesIO (correct for binary files)\n",
    "        # Dont use StringIO(), which is meant for handling text data (like CSV)\n",
    "        buffer = io.BytesIO()\n",
    "        df_year.to_parquet(buffer, index=False, engine=\"pyarrow\")\n",
    "\n",
    "        # Move buffer position to the beginning\n",
    "        buffer.seek(0)\n",
    "\n",
    "        # Define S3 key (file path)\n",
    "        parquet_key = f\"{partitioned_folder}season={year}/data.parquet\"\n",
    "\n",
    "        # Upload to S3\n",
    "        s3_resource.Object(s3_bucket, parquet_key).put(Body=buffer)\n",
    "\n",
    "        print(f\"Uploaded {parquet_key} to S3 ✅\")\n",
    "\n",
    "                # Also upload the first file to the analysis folder\n",
    "        if first:\n",
    "            buffer.seek(0)  # Reset buffer for reuse\n",
    "\n",
    "            with open(local_parquet_file_path, \"wb\") as f:\n",
    "                f.write(buffer.read())\n",
    "            print(f\"Also saved first Parquet file locally to {local_parquet_file_path} ✅\")\n",
    "\n",
    "            first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_schema_sql(local_parquet_file_path):\n",
    "\n",
    "    df = pd.read_parquet(local_parquet_file_path)\n",
    "\n",
    "    dtype_mapping = {\n",
    "        'Int64': 'INT',\n",
    "        'object': 'VARCHAR(100)',\n",
    "        'string': 'VARCHAR(100)',\n",
    "    }\n",
    "\n",
    "    columns_sql = []\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        athena_type = dtype_mapping.get(dtype, 'VARCHAR(100)')  # default fallback\n",
    "        columns_sql.append(f\"{col} {athena_type}\")\n",
    "\n",
    "    # Join into a single string for the CREATE TABLE query\n",
    "    schema_sql_temp = \",\\n    \".join(columns_sql)\n",
    "    add_start_tab = '    '\n",
    "    add_end_line_break = '\\n'\n",
    "    table_schema_sql = add_start_tab + schema_sql_temp + add_end_line_break\n",
    "\n",
    "    return table_schema_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CSV to parquet partitions\n",
    "s3_bucket = \"chalkjuice-backend\"                              # Csv bucket\n",
    "file_name = \"nfl_games_all.csv\"                               # Csv file name\n",
    "data_folder_name = 'data'\n",
    "\n",
    "partition_by = \"season\"                                       # Define column to partition by \n",
    "partitioned_folder = \"nfl_games_all_partitions/\"              # Define new S3 folder for new partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Athena Table From Parquet\n",
    "athena_database = 'nfl'\n",
    "athena_table = 'nfl_games_all'\n",
    "athena_output_folder = 'nfl_games_all_athena_parquet/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logged in to ECR successfully.\n",
      "✅ Connected to all clients successfully.\n"
     ]
    }
   ],
   "source": [
    "# Connect to custom AWS class\n",
    "jb = joel_boto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UserId': 'AIDAQMEY5XVU2GWNZLHO5',\n",
       " 'Account': '026090519913',\n",
       " 'Arn': 'arn:aws:iam::026090519913:user/ChalkJuice',\n",
       " 'ResponseMetadata': {'RequestId': '03849057-66d5-4064-a9f5-641b13289d34',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '03849057-66d5-4064-a9f5-641b13289d34',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '407',\n",
       "   'date': 'Mon, 09 Jun 2025 21:33:46 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_client = jb.session.client('sts')\n",
    "identity = sts_client.get_caller_identity()\n",
    "account_id = identity['Account']\n",
    "identity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'026090519913'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jb.account_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV to parquet partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If csv in s3 => continue. If in the local repo => create new bucket and upload the local csv file to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Exists\n"
     ]
    }
   ],
   "source": [
    "if jb.s3_bucket_exists(s3_bucket):\n",
    "    print('Bucket Exists')\n",
    "    pass\n",
    "else:\n",
    "    # Create a new s3 bucket\n",
    "    jb.create_s3_bucket(s3_bucket)\n",
    "\n",
    "    # Upload the CSV to s3\n",
    "    local_csv_file_path = os.path.join(\"..\", data_folder_name, file_name)\n",
    "    jb.upload_file_to_s3(local_csv_file_path, s3_bucket, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return the s3 csv as a pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = jb.s3_csv_to_df(s3_bucket, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure column datatypes for Athena - its fussy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_dtypes_for_athena(df)\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create partitioned Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded nfl_games_all_partitions/season=1967/data.parquet to S3 ✅\n",
      "Also saved first Parquet file locally to ..\\data\\nfl_games_all.parquet ✅\n",
      "Uploaded nfl_games_all_partitions/season=1968/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1969/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1970/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1971/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1972/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1973/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1974/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1975/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1976/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1977/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1978/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1979/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1980/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1981/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1982/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1983/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1984/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1985/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1986/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1987/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1988/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1989/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1990/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1991/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1992/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1993/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1994/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1995/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1996/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1997/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1998/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1999/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2000/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2001/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2002/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2003/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2004/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2005/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2006/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2007/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2008/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2009/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2010/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2011/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2012/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2013/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2014/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2015/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2016/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2017/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2018/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2019/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2020/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2021/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2022/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2023/data.parquet to S3 ✅\n"
     ]
    }
   ],
   "source": [
    "local_parquet_file_path = os.path.join(\"..\", data_folder_name, f\"{file_name[:-4]}.parquet\") # OPTIONAL save one parquet file locally to autogenerate athena table schema\n",
    "df_to_s3_parquet(jb.s3_resource, df, s3_bucket, partition_by, partitioned_folder, local_parquet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Athena Table From Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create database if not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query finished with status: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'56984afd-ba5f-4970-b7a0-1eb576d5dff1'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_db_query = f\"CREATE DATABASE IF NOT EXISTS {athena_database}\"\n",
    "jb.query_athena(create_db_query, athena_database, athena_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove table if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query finished with status: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'27c5f3c0-dc1b-45b7-9d32-90af57a83c6d'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_table_query = f'DROP TABLE IF EXISTS {athena_database}.{athena_table};'\n",
    "jb.query_athena(remove_table_query, athena_database, athena_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query finished with status: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'32fccf62-4820-4f97-a6e7-6c6e3a16e347'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_schema_sql = generate_table_schema_sql(local_parquet_file_path)\n",
    "create_and_fill_table_query = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {athena_database}.{athena_table} (\n",
    "    {table_schema_sql}\n",
    "    )\n",
    "    PARTITIONED BY ({partition_by} INT)\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 's3://{s3_bucket}/{partitioned_folder}'\n",
    "    TBLPROPERTIES (\n",
    "        'parquet.compression'='SNAPPY',\n",
    "        'projection.enabled'='true',\n",
    "        'projection.{partition_by}.type'='integer',\n",
    "        'projection.{partition_by}.range'='1967,2023',\n",
    "        'storage.location.template'='s3://{s3_bucket}/{partitioned_folder}{partition_by}=${{{partition_by}}}/'\n",
    "    );\n",
    "\"\"\"\n",
    "jb.query_athena(create_and_fill_table_query, athena_database, athena_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into the table and map the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query finished with status: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e446c3d0-9886-46d2-9bf6-8ed682ce3416'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_partitions_query = f'MSCK REPAIR TABLE {athena_database}.{athena_table};'\n",
    "jb.query_athena(map_partitions_query, athena_database, athena_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = \"MIN\"\n",
    "year1, year2, year3 = 2021, 2022, 2023  # Adjust years as needed\n",
    "test_athena_connection_query = f'''\n",
    "    SELECT * FROM \"{athena_database}\".\"{athena_table}\" \n",
    "    WHERE season IN ({year1}, {year2}, {year3});\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query finished with status: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": " Col0",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0ec6fc14-01f6-4848-a00c-804fe5079406",
       "rows": [
        [
         "0",
         "26206"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Col0\n",
       "0  26206"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_execution_id = jb.query_athena('SELECT COUNT(*) FROM \"nfl\".\"nfl_games_all\";', jb.athena_client, athena_database, athena_output_folder)\n",
    "df = jb.create_df_from_athena_query(query_execution_id)\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
