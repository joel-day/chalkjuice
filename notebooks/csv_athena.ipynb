{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV to Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSV to parquet partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boto3_session():\n",
    "    # Load .env file variables into the environment\n",
    "    load_dotenv()\n",
    "    aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    aws_region = os.getenv(\"AWS_DEFAULT_REGION\")\n",
    "\n",
    "    # Safety check\n",
    "    if not aws_access_key or not aws_secret_key:\n",
    "        raise ValueError(\"Missing AWS credentials in .env file.\")\n",
    "\n",
    "    # Create boto3 session\n",
    "    boto3_session = boto3.Session(\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key,\n",
    "        region_name=aws_region\n",
    "    )\n",
    "\n",
    "    return boto3_session, aws_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the specified file from s3 and loads into pandas df\n",
    "def connect_to_s3(boto3_session): \n",
    "    # Create an S3 client\n",
    "    s3_client = boto3_session.client(\"s3\")\n",
    "\n",
    "    # Create S3 resource\n",
    "    s3_resource = boto3_session.resource(\"s3\")\n",
    "\n",
    "    return s3_client, s3_resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_exists(bucket_name: str) -> bool:\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket_name)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = int(e.response[\"Error\"][\"Code\"])\n",
    "        if error_code == 404:\n",
    "            return False  # Bucket does not exist\n",
    "        elif error_code == 403:\n",
    "            return True   # Bucket exists but is not accessible (you don't own it)\n",
    "        else:\n",
    "            return False  # Other error (e.g., network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(s3_client, bucket_name, region):\n",
    "    try:\n",
    "        s3_client.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={'LocationConstraint': region}\n",
    "        )\n",
    "        print(f\"✅ Created bucket: {bucket_name}\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"BucketAlreadyOwnedByYou\":\n",
    "            print(f\"✅ Bucket already exists and owned by you: {bucket_name}\")\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_s3(s3_client, local_csv_file_path, bucket_name, file_name):\n",
    "    try:\n",
    "        s3_client.upload_file(local_csv_file_path, bucket_name, file_name)\n",
    "        print(f\"✅ Uploaded {local_csv_file_path} to s3://{bucket_name}/{file_name}\")\n",
    "    except ClientError as e:\n",
    "        print(f\"❌ Upload failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the specified file from s3 and loads into pandas df\n",
    "def s3_csv_to_df(s3_client, s3_bucket, file_name): \n",
    "    # Read CSV from S3 int pandas\n",
    "    obj = s3_client.get_object(Bucket=s3_bucket, Key=file_name)\n",
    "    df = pd.read_csv(obj[\"Body\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle data types for athena\n",
    "def convert_dtypes_for_athena(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        dtype = df[col].dtype\n",
    "        \n",
    "        if dtype in [\"int64\", \"float64\"]:\n",
    "            df[col] = df[col].astype(\"Int64\")  # Nullable integer\n",
    "        elif \"date\" in col_lower:\n",
    "            df[col] = df[col].astype(\"object\")  # Will convert to datetime64[ns]\n",
    "        elif dtype == 'object':\n",
    "            df[col] = df[col].astype(\"string\")  # Use StringDtype\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates parguet files from a df with specific partitioning and destination location\n",
    "def df_to_s3_parquet(s3_resource, df, s3_bucket, partition_by, partitioned_folder, local_parquet_file_path):\n",
    "    first = True  # Flag to track the first file\n",
    "\n",
    "    # Loop through each unique year\n",
    "    for year in df[partition_by].unique():\n",
    "        # Filter data for that year\n",
    "        df_year = df[df[partition_by] == year]\n",
    "\n",
    "        df_year = df_year.drop(columns=[partition_by])\n",
    "\n",
    "        # Convert to Parquet using BytesIO (correct for binary files)\n",
    "        # Dont use StringIO(), which is meant for handling text data (like CSV)\n",
    "        buffer = io.BytesIO()\n",
    "        df_year.to_parquet(buffer, index=False, engine=\"pyarrow\")\n",
    "\n",
    "        # Move buffer position to the beginning\n",
    "        buffer.seek(0)\n",
    "\n",
    "        # Define S3 key (file path)\n",
    "        parquet_key = f\"{partitioned_folder}season={year}/data.parquet\"\n",
    "\n",
    "        # Upload to S3\n",
    "        s3_resource.Object(s3_bucket, parquet_key).put(Body=buffer)\n",
    "\n",
    "        print(f\"Uploaded {parquet_key} to S3 ✅\")\n",
    "\n",
    "                # Also upload the first file to the analysis folder\n",
    "        if first:\n",
    "            buffer.seek(0)  # Reset buffer for reuse\n",
    "\n",
    "            with open(local_parquet_file_path, \"wb\") as f:\n",
    "                f.write(buffer.read())\n",
    "            print(f\"Also saved first Parquet file locally to {local_parquet_file_path} ✅\")\n",
    "\n",
    "            first = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Athena table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_athena(query, athena, athena_database, athena_output_location):\n",
    "    response = athena.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database': athena_database},\n",
    "        ResultConfiguration={'OutputLocation': athena_output_location}\n",
    "    )\n",
    "\n",
    "    # Wait for the query to finish\n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "    while True:\n",
    "        status = athena.get_query_execution(QueryExecutionId=query_execution_id)['QueryExecution']['Status']['State']\n",
    "        if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "            print(f\"Query finished with status: {status}\")\n",
    "            break\n",
    "        time.sleep(1)\n",
    "\n",
    "    if status != 'SUCCEEDED':\n",
    "        raise Exception(\"Failed to query database.\")\n",
    "    \n",
    "    return query_execution_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_athena_query(query_execution_id, athena_client):\n",
    "    \n",
    "    # Get Query Results\n",
    "    results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "\n",
    "    columns = [col[\"Label\"] for col in results[\"ResultSet\"][\"ResultSetMetadata\"][\"ColumnInfo\"]]\n",
    "\n",
    "    # Extract Rows\n",
    "    rows = []\n",
    "    for row in results[\"ResultSet\"][\"Rows\"][1:]:  # Skip header row\n",
    "        extracted_row = [col.get(\"VarCharValue\", None) for col in row[\"Data\"]]  # Extract actual values\n",
    "        rows.append(extracted_row)\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df = df.fillna(\"NA\")\n",
    "    df.columns = df.columns.str.replace('_', ' ').str.title()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_schema_sql(local_parquet_file_path):\n",
    "\n",
    "    df = pd.read_parquet(local_parquet_file_path)\n",
    "\n",
    "    dtype_mapping = {\n",
    "        'Int64': 'INT',\n",
    "        'object': 'VARCHAR(100)',\n",
    "        'string': 'VARCHAR(100)',\n",
    "    }\n",
    "\n",
    "    columns_sql = []\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        athena_type = dtype_mapping.get(dtype, 'VARCHAR(100)')  # default fallback\n",
    "        columns_sql.append(f\"{col} {athena_type}\")\n",
    "\n",
    "    # Join into a single string for the CREATE TABLE query\n",
    "    schema_sql_temp = \",\\n    \".join(columns_sql)\n",
    "    add_start_tab = '    '\n",
    "    add_end_line_break = '\\n'\n",
    "    table_schema_sql = add_start_tab + schema_sql_temp + add_end_line_break\n",
    "\n",
    "    return table_schema_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CSV to parquet partitions\n",
    "s3_bucket = \"chalkjuice-backend\"                              # Csv bucket\n",
    "file_name = \"nfl_games_all.csv\"                               # Csv file name\n",
    "data_folder_name = 'data'\n",
    "\n",
    "partition_by = \"season\"                                       # Define column to partition by \n",
    "partitioned_folder = \"nfl_games_all_partitions/\"              # Define new S3 folder for new partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Athena Table From Parquet\n",
    "athena_database = 'nfl'\n",
    "athena_table = 'nfl_games_all'\n",
    "athena_output_folder = 'nfl_games_all_athena_parquet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV to parquet partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to S3 with boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_session, aws_region = create_boto3_session()\n",
    "s3_client, s3_resource = connect_to_s3(boto3_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If csv in s3 => continue. If in the local repo => create new bucket and upload the local csv file to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created bucket: chalkjuice-backend\n",
      "✅ Uploaded ..\\data\\nfl_games_all.csv to s3://chalkjuice-backend/nfl_games_all.csv\n"
     ]
    }
   ],
   "source": [
    "if bucket_exists(\"chalkjuice-backend\"):\n",
    "    print('Bucket Exists')\n",
    "    pass\n",
    "else:\n",
    "    # Create a new s3 bucket\n",
    "    create_bucket(s3_client, s3_bucket, aws_region)\n",
    "\n",
    "    # Upload the CSV to s3\n",
    "    local_csv_file_path = os.path.join(\"..\", data_folder_name, file_name)\n",
    "    upload_file_to_s3(s3_client, local_csv_file_path, s3_bucket, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return the s3 csv as a pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = s3_csv_to_df(s3_client, s3_bucket, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure column datatypes for Athena - its fussy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = convert_dtypes_for_athena(df)\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create partitioned Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded nfl_games_all_partitions/season=1967/data.parquet to S3 ✅\n",
      "Also saved first Parquet file locally to ..\\data\\nfl_games_all.parquet ✅\n",
      "Uploaded nfl_games_all_partitions/season=1968/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1969/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1970/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1971/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1972/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1973/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1974/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1975/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1976/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1977/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1978/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1979/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1980/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1981/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1982/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1983/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1984/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1985/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1986/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1987/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1988/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1989/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1990/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1991/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1992/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1993/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1994/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1995/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1996/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1997/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1998/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=1999/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2000/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2001/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2002/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2003/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2004/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2005/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2006/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2007/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2008/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2009/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2010/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2011/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2012/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2013/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2014/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2015/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2016/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2017/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2018/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2019/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2020/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2021/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2022/data.parquet to S3 ✅\n",
      "Uploaded nfl_games_all_partitions/season=2023/data.parquet to S3 ✅\n"
     ]
    }
   ],
   "source": [
    "local_parquet_file_path = os.path.join(\"..\", data_folder_name, f\"{file_name[:-4]}.parquet\") # OPTIONAL save one parquet file locally to autogenerate athena table schema\n",
    "df_to_s3_parquet(s3_resource, df, s3_bucket, partition_by, partitioned_folder, local_parquet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Athena Table From Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_output_location = f's3://{s3_bucket}/{athena_output_folder}'\n",
    "athena_client = boto3_session.client('athena')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create database if not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query finished with status: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'56984afd-ba5f-4970-b7a0-1eb576d5dff1'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_db_query = f\"CREATE DATABASE IF NOT EXISTS {athena_database}\"\n",
    "query_athena(create_db_query, athena_client, athena_database, athena_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove table if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query finished with status: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'27c5f3c0-dc1b-45b7-9d32-90af57a83c6d'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_table_query = f'DROP TABLE IF EXISTS {athena_database}.{athena_table};'\n",
    "query_athena(remove_table_query, athena_client, athena_database, athena_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query finished with status: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'32fccf62-4820-4f97-a6e7-6c6e3a16e347'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_schema_sql = generate_table_schema_sql(local_parquet_file_path)\n",
    "create_and_fill_table_query = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {athena_database}.{athena_table} (\n",
    "    {table_schema_sql}\n",
    "    )\n",
    "    PARTITIONED BY ({partition_by} INT)\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 's3://{s3_bucket}/{partitioned_folder}'\n",
    "    TBLPROPERTIES (\n",
    "        'parquet.compression'='SNAPPY',\n",
    "        'projection.enabled'='true',\n",
    "        'projection.{partition_by}.type'='integer',\n",
    "        'projection.{partition_by}.range'='1967,2023',\n",
    "        'storage.location.template'='s3://{s3_bucket}/{partitioned_folder}{partition_by}=${{{partition_by}}}/'\n",
    "    );\n",
    "\"\"\"\n",
    "query_athena(create_and_fill_table_query, athena_client, athena_database, athena_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into the table and map the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query finished with status: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e446c3d0-9886-46d2-9bf6-8ed682ce3416'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_partitions_query = f'MSCK REPAIR TABLE {athena_database}.{athena_table};'\n",
    "query_athena(map_partitions_query, athena_client, athena_database, athena_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = \"MIN\"\n",
    "year1, year2, year3 = 2021, 2022, 2023  # Adjust years as needed\n",
    "test_athena_connection_query = f'''\n",
    "    SELECT * FROM \"{athena_database}\".\"{athena_table}\" \n",
    "    WHERE season IN ({year1}, {year2}, {year3});\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query finished with status: SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": " Col0",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0ec6fc14-01f6-4848-a00c-804fe5079406",
       "rows": [
        [
         "0",
         "26206"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Col0\n",
       "0  26206"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_execution_id = query_athena('SELECT COUNT(*) FROM \"nfl\".\"nfl_games_all\";', athena_client, athena_database, athena_output_location)\n",
    "df = create_df_from_athena_query(query_execution_id, athena_client)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
